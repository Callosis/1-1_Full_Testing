{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['input', 'lib', 'working', 'src']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import gc\n",
    "import numba\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "#Initialize Variables\n",
    "np.random.seed(1)\n",
    "alpha = .05\n",
    "omega = 0\n",
    "smtooh_thresh = .5\n",
    "\n",
    "network_num = 3\n",
    "h_layer_num = 6\n",
    "h_layere_num = 2\n",
    "\n",
    "n_x = 800000\n",
    "n_h = 400\n",
    "n_y = 100\n",
    "layers_dims = [n_x, n_y, n_h, h_layer_num]\n",
    "\n",
    "ne_x = 300\n",
    "ne_h = 100\n",
    "ne_y = 1\n",
    "layere_dims = [ne_x, ne_y, ne_h, h_layere_num]\n",
    "\n",
    "\n",
    "num_batches = 12\n",
    "batchsize_max = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "70e81ca35a567ba10abfa2ff86f7a89daeeefafa"
   },
   "outputs": [],
   "source": [
    "#TODO 1.1\n",
    "#Create full output for submissions\n",
    "#Add convienence functions, plotting, type changes, etc.\n",
    "#Add competition result evaluation function\n",
    "#Set logic into classes?\n",
    "\n",
    "\n",
    "#TODO 1.2\n",
    "#initialize different random seed each time, use mod of time?\n",
    "#change layer_dims to allow variable layers per unit\n",
    "#change to new structure\n",
    "#Add inhibitory functions\n",
    "#Change to RELU activation\n",
    "#allow for activations function change?\n",
    "#check to make sure Y1[x] = Y2[x]\n",
    "\n",
    "#TODO 1.3\n",
    "#Harp style connections, angled out for access to outputs\n",
    "\n",
    "#TODO for LTSM \n",
    "#change batch numbers to random number between 0 and total batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5c97ac625e837eaa63c1e845f7c70c9d28298fa4"
   },
   "outputs": [],
   "source": [
    "#Format Input Training Data\n",
    "def retrieve_traindata():\n",
    "    rawmeta_train = pd.read_csv('../input/metadata_train.csv', dtype='int8')\n",
    "    ph1_list = rawmeta_train.loc[rawmeta_train['phase'] == 0].copy()\n",
    "    ph2_list = rawmeta_train.loc[rawmeta_train['phase'] == 1].copy()\n",
    "    ph3_list = rawmeta_train.loc[rawmeta_train['phase'] == 2].copy()\n",
    "    \n",
    "    rawdata_train = pq.read_pandas('../input/train.parquet').to_pandas()\n",
    "    ph1data_train = rawdata_train.iloc[:,ph1_list['signal_id'].values].copy()\n",
    "    ph2data_train = rawdata_train.iloc[:,ph2_list['signal_id'].values].copy()\n",
    "    ph3data_train = rawdata_train.iloc[:,ph3_list['signal_id'].values].copy()\n",
    "    del rawdata_train\n",
    "    Y1 = ph1_list['target'].copy()\n",
    "    Y2 = ph2_list['target'].copy()\n",
    "    Y3 = ph3_list['target'].copy()\n",
    "    \n",
    "    del ph1_list, ph2_list, ph3_list, rawmeta_train\n",
    "    gc.collect()\n",
    "\n",
    "    return ph1data_train, ph2data_train, ph3data_train, Y1, Y2, Y3\n",
    "\n",
    "\n",
    "\n",
    "#Separate Training Data into Batches\n",
    "def segment_traindata(num_batches, batchsize_max):\n",
    "    try:\n",
    "        os.mkdir('../train_batches')\n",
    "    except FileNotFoundError:\n",
    "        #file not found\n",
    "        print(\"File not found, exiting...\")\n",
    "    else:\n",
    "        ph1_data, ph2_data, ph3_data, train_Y1, train_Y2, train_Y3 = retrieve_traindata()\n",
    "        \n",
    "        n_x, m = ph1_data.shape\n",
    "        mlist = list(range(0, m))\n",
    "        \n",
    "        ph1_data.columns = mlist\n",
    "        train_Y1.index = mlist\n",
    "        \n",
    "        ph2_data.columns = mlist\n",
    "        train_Y2.index = mlist\n",
    "        \n",
    "        ph3_data.columns = mlist\n",
    "        train_Y3.index = mlist\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_size = np.random.randint(batchsize_max)\n",
    "            batch_start = m\n",
    "            \n",
    "            while batch_start >= (m - batch_size):\n",
    "                batch_start = np.random.randint(m)\n",
    "            batch_end = batch_start+batch_size\n",
    "            \n",
    "            \n",
    "            batch1 = ph1_data.iloc[:,batch_start:batch_end].copy()\n",
    "            batch2 = ph2_data.iloc[:,batch_start:batch_end].copy()\n",
    "            batch3 = ph3_data.iloc[:,batch_start:batch_end].copy()\n",
    "        \n",
    "            Y1 = train_Y1.iloc[batch_start:batch_end].copy()\n",
    "            Y2 = train_Y2.iloc[batch_start:batch_end].copy()\n",
    "            Y3 = train_Y3.iloc[batch_start:batch_end].copy()\n",
    "        \n",
    "            batch1.to_csv('../train_batches/ph1_' + str(i)+\".csv\")\n",
    "            batch2.to_csv('../train_batches/ph2_' + str(i)+\".csv\")\n",
    "            batch3.to_csv('../train_batches/ph3_' + str(i)+\".csv\")\n",
    "        \n",
    "            Y1.to_csv('../train_batches/y1_' + str(i)+\".csv\", header = None)\n",
    "            Y2.to_csv('../train_batches/y2_' + str(i)+\".csv\", header = None)\n",
    "            Y3.to_csv('../train_batches/y3_' + str(i)+\".csv\", header = None)\n",
    "            \n",
    "            \n",
    "            print(\"Batch \"+str(i+1)+\" of \"+str(num_batches))\n",
    "            #print(\"batch1: \", batch1[0:10])\n",
    "            #print(\"Y1: \", Y1[0:10])\n",
    "            #print(\"Batch size: \", batch_size)\n",
    "            #print(\"Batch start: \", batch_start)\n",
    "            #print(\"Batch end: \", batch_end)\n",
    "            #print(\"batch1: \", batch1.shape)\n",
    "            #print(\"Y1: \", Y1.shape)\n",
    "            #del batch1, batch2, batch3, Y1, Y2, Y3\n",
    "            #gc.collect()\n",
    "    \n",
    "    del ph1_data, ph2_data, ph3_data, train_Y1, train_Y2, train_Y3\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "\n",
    "#Convert CSV File Data into Numpy Array\n",
    "def filetondarray(filename_data, filename_targets):\n",
    "    data = pd.read_csv(filename_data, dtype='int8')\n",
    "    Y = pd.read_csv(filename_targets, dtype='int8', header=None)\n",
    "    #print(Y.head)\n",
    "    #print(data.shape)\n",
    "    #print(Y.shape)\n",
    "    data_out = data.iloc[:,1:].values\n",
    "    Y_out = Y.iloc[:,1:].values\n",
    "    \n",
    "    \n",
    "    return data_out.copy(), Y_out.T.copy()\n",
    "\n",
    "#Cleanup File Directory of old Batched Data\n",
    "def file_cleanup():\n",
    "    folder = '../train_batches/'\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "    #print(os.listdir(\"../train_batches\"))\n",
    "    \n",
    "    os.rmdir('../train_batches')\n",
    "    return\n",
    "\n",
    "#Format Input Testing Data\n",
    "def retrieve_testdata():\n",
    "    rawmeta_test = pd.read_csv('../input/metadata_test.csv', dtype='int8')\n",
    "    ph1_list = rawmeta_test.loc[rawmeta_test['phase'] == 0].copy()\n",
    "    ph2_list = rawmeta_test.loc[rawmeta_test['phase'] == 1].copy()\n",
    "    ph3_list = rawmeta_test.loc[rawmeta_test['phase'] == 2].copy()\n",
    "    \n",
    "    rawdata_test = pq.read_pandas('../input/test.parquet').to_pandas()\n",
    "    ph1data_test = rawdata_test.iloc[:,ph1_list['signal_id'].values].copy()\n",
    "    ph2data_test = rawdata_test.iloc[:,ph2_list['signal_id'].values].copy()\n",
    "    ph3data_test = rawdata_test.iloc[:,ph3_list['signal_id'].values].copy()\n",
    "    del rawdata_train\n",
    "    Y1 = ph1_list['target'].copy()\n",
    "    Y2 = ph2_list['target'].copy()\n",
    "    Y3 = ph3_list['target'].copy()\n",
    "    \n",
    "    del ph1_list, ph2_list, ph3_list, rawmeta_test\n",
    "    gc.collect()\n",
    "\n",
    "    return ph1data_test, ph2data_test, ph3data_test, Y1, Y2, Y3\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layers_dims):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    n_x, n_y, n_h, num_h_layers = layers_dims\n",
    "    \n",
    "    W1 = 0.01 * np.random.randn(n_h, n_x).astype(np.float32)\n",
    "    b1 = np.zeros((n_h, 1)).astype(np.float32)\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1}\n",
    "        \n",
    "    for i in range(2,num_h_layers):\n",
    "        W = np.random.randn(n_h, n_h).astype(np.float32) * 0.01\n",
    "        b = np.zeros((n_h, 1)).astype(np.float32)\n",
    "    \n",
    "        assert (W.shape == (n_h, n_h))\n",
    "        assert (b.shape == (n_h, 1))\n",
    "        \n",
    "        \n",
    "        parameters['W'+str(i)] = W\n",
    "        parameters['b'+str(i)] = b\n",
    "    \n",
    "    W_last = np.random.randn(n_y, n_h).astype(np.float32) * 0.01\n",
    "    b_last = np.zeros((n_y, 1)).astype(np.float32)\n",
    "    \n",
    "    assert (W_last.shape == (n_y, n_h))\n",
    "    assert (b_last.shape == (n_y, 1))\n",
    "\n",
    "        \n",
    "    parameters['W'+str(num_h_layers)] = W_last\n",
    "    parameters['b'+str(num_h_layers)] = b_last\n",
    "    \n",
    "    #del W_last, b_last, W, b, W1, b1, i\n",
    "    #gc.collect()\n",
    "    \n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def L_model_forward(X, parameters, num_h_layers):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    \n",
    "    Z1 = np.dot(parameters['W1'],X) + parameters[\"b1\"]\n",
    "    A1 = np.tanh(Z1)\n",
    "    #print(\"W1 shape: \", parameters['W1'].shape)\n",
    "    #print(\"b1 shape: \", parameters[\"b1\"].shape)\n",
    "    #print(\"A1 shape: \", A1.shape)\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1}\n",
    "    A_prev = A1\n",
    "    for i in range(2, num_h_layers):\n",
    "        # Retrieve each parameter from the dictionary \"parameters\"\n",
    "        cache['Z'+str(i)] = np.dot(parameters[\"W\"+str(i)],A_prev) + parameters[\"b\"+str(i)]\n",
    "        cache['A'+str(i)] = np.tanh(cache['Z'+str(i)])\n",
    "        A_prev = cache['A'+str(i)]\n",
    "        #print(\"W\"+str(i)+\" shape: \", parameters[\"W\"+str(i)].shape)\n",
    "        #print(\"b\"+str(i)+\" shape: \", parameters[\"b\"+str(i)].shape)\n",
    "        #print(\"A\"+str(i)+\" shape: \", cache['A'+str(i)].shape)\n",
    "        \n",
    "    cache['Z'+str(num_h_layers)] = np.dot(parameters[\"W\"+str(num_h_layers)],A_prev) + parameters[\"b\"+str(num_h_layers)]\n",
    "    cache['A'+str(num_h_layers)] = sigmoid(cache['Z'+str(num_h_layers)])\n",
    "    \n",
    "    #print(\"W\"+str(num_h_layers)+\" shape: \", parameters[\"W\"+str(num_h_layers)].shape)\n",
    "    #print(\"b\"+str(num_h_layers)+\" shape: \", parameters[\"b\"+str(num_h_layers)].shape)\n",
    "    #print(\"A\"+str(num_h_layers)+\" shape: \", cache['A'+str(num_h_layers)].shape)\n",
    "    \n",
    "    \n",
    "    #del A_prev, A1, Z1, i\n",
    "    #gc.collect()\n",
    "    \n",
    "    return cache['A'+str(num_h_layers)], cache\n",
    "\n",
    "\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = (np.multiply(Y,np.log(A2))+(np.multiply((1-Y),(np.log(1-A2)))))\n",
    "    cost = (-1/m) * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    del logprobs, m\n",
    "    gc.collect()\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def L_model_backward(parameters, cache, X, Y, num_h_layers, dZ_next = [], network_bridge=0):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Backward propagation: calculate gradients.\n",
    "    if network_bridge == 1:\n",
    "        dZ_last = (np.dot(Y.T,dZ_next)) * (1 - np.power(cache[\"A\"+str(num_h_layers)], 2))\n",
    "    else:\n",
    "        dZ_last = cache[\"A\"+str(num_h_layers)] - Y\n",
    "    dW_last = (1/m) * np.dot(dZ_last, cache[\"A\"+str(num_h_layers-1)].T)\n",
    "    db_last = (1/m) * np.sum(dZ_last, axis=1, keepdims=True)\n",
    "    dZ_next = dZ_last\n",
    "    dW_next = dW_last\n",
    "    db_next = db_last\n",
    "    grads = {\"dW\"+str(num_h_layers) : dW_last,\n",
    "             \"db\"+str(num_h_layers) : db_last}\n",
    "    #print(\"dW\"+str(num_h_layers)+\" shape: \", grads[\"dW\"+str(num_h_layers)].shape)\n",
    "    #print(\"db\"+str(num_h_layers)+\" shape: \", grads[\"db\"+str(num_h_layers)].shape)\n",
    "    if num_h_layers > 2:\n",
    "        for i in range(1, num_h_layers):\n",
    "            j = num_h_layers - i\n",
    "            dZ = (np.dot(parameters[\"W\"+str(j+1)].T,dZ_next)) * (1 - np.power(cache[\"A\"+str(j)], 2))\n",
    "            dW = (1/m) * np.dot(dZ, cache[\"A\"+str(j)].T)\n",
    "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            dZ_next = dZ\n",
    "            dW_next = dW\n",
    "            db_next = db\n",
    "            grads[\"dW\"+str(j)] = dW\n",
    "            grads[\"db\"+str(j)] = db\n",
    "            #print(\"dW\"+str(j)+\" shape: \", grads[\"dW\"+str(j)].shape)\n",
    "            #print(\"db\"+str(j)+\" shape: \", grads[\"db\"+str(j)].shape)\n",
    "        \n",
    "        \n",
    "    dZ1 = (np.dot(parameters[\"W\"+str(2)].T,dZ_next)) * (1 - np.power(cache[\"A\"+str(1)], 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads[\"dW1\"] = dW1\n",
    "    grads[\"db1\"] = db1\n",
    "    \n",
    "    #print(\"dW\"+str(1)+\" shape: \", grads[\"dW\"+str(1)].shape)\n",
    "    \n",
    "    del dZ_last, dW_last, db_last, dZ_next, dW_next, db_next, dW1, db1\n",
    "    gc.collect()\n",
    "    \n",
    "    if network_bridge == 2:\n",
    "        return grads, parameters[\"W1\"], dZ1\n",
    "    else:\n",
    "        del dZ1\n",
    "        gc.collect()\n",
    "        return grads\n",
    "\n",
    "def update_parameters(parameters, grads, num_h_layers, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    for i in range(1, num_h_layers+1):\n",
    "    \n",
    "        # Update rule for each parameter\n",
    "        parameters[\"W\"+str(i)] = parameters[\"W\"+str(i)] - (learning_rate * grads[\"dW\"+str(i)])\n",
    "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - (learning_rate * grads[\"db\"+str(i)])\n",
    "    \n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > .5)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def l_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->TANH]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> TANH]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters, layers_dims[3])\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(parameters, caches, X, Y, layers_dims[3])\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, layers_dims[3], learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "    print(\"Outside of Update for loop\")\n",
    "    print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def l_triplelayer_trainmodel(batch_num, layers_dims, layere_dims, learning_rate = 0.005, learning_Erate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->TANH]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. \n",
    "    parameters1 = initialize_parameters_deep(layers_dims)\n",
    "    parameters2 = initialize_parameters_deep(layers_dims)\n",
    "    parameters3 = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    parametersE = initialize_parameters_deep(layere_dims)\n",
    "    \n",
    "    batch_i = 0 #switch out batches of training data\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        if i % 10 == 0 or i==0:\n",
    "            if batch_i == batch_num:\n",
    "                batch_i = 0\n",
    "            print(\"Batch Num %i: %i\" %(batch_i, batch_i))\n",
    "            ph1_train, Y = filetondarray(\"../train_batches/ph1_\"+batch_i+\".csv\", \"../train_batches/y1_\"+batch_i+\".csv\")\n",
    "            ph2_train, Y2 = filetondarray(\"../train_batches/ph2_\"+batch_i+\".csv\", \"../train_batches/y2_\"+batch_i+\".csv\")\n",
    "            ph3_train, Y3 = filetondarray(\"../train_batches/ph3_\"+batch_i+\".csv\", \"../train_batches/y3_\"+batch_i+\".csv\")\n",
    "            batch_i += 1\n",
    "\n",
    "        # Forward propagation for the first 3 input networks: [LINEAR -> TANH]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        #print(\"Phase 1 Network\")\n",
    "        AL1, caches1 = L_model_forward(X1, parameters1, layers_dims[3])\n",
    "        #print(\"AL1 shape: \", AL1.shape)\n",
    "        #print(\"Phase 2 Network\")\n",
    "        AL2, caches2 = L_model_forward(X2, parameters2, layers_dims[3])\n",
    "        #print(\"AL2 shape: \", AL2.shape)\n",
    "        #print(\"Phase 3 Network\")\n",
    "        AL3, caches3 = L_model_forward(X3, parameters3, layers_dims[3])\n",
    "        #print(\"AL3 shape: \", AL3.shape)\n",
    "        \n",
    "        \"\"\"Create intermediate array to feed combining network, 3 networks into one\n",
    "            N1 ->  |\n",
    "            N2 ->  | -> AX -> Ne\n",
    "            N3 ->  |\n",
    "            \"\"\"\n",
    "        \n",
    "        AX = np.zeros((layere_dims[0], AL1.shape[1]), dtype=np.float32)\n",
    "        AX[:AL1.shape[0],:] = AL1\n",
    "        AX[AL1.shape[0]:AL2.shape[0]+AL1.shape[0],:] = AL2\n",
    "        AX[AL2.shape[0]+AL1.shape[0]:AL3.shape[0]+AL2.shape[0]+AL1.shape[0],:] = AL3\n",
    "        #print('AX shape: ', AX.shape)\n",
    "        \n",
    "        #Forward Propaget combined network segment\n",
    "        #print(\"Final Layer Shape\")\n",
    "        ALe, cachesE = L_model_forward(AX, parametersE, layere_dims[3])\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(ALe, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        #print(\"Final layer gradients\")\n",
    "        gradsE, Wp, dZ = L_model_backward(parametersE, cachesE, AX, Y, layere_dims[3], network_bridge=2)\n",
    "        #print(\"Wp shape: \", Wp.shape)\n",
    "        #print(\"dZ shape: \", dZ.shape)\n",
    "        \n",
    "        \n",
    "        #print(\"Phase 1 gradients\")\n",
    "        grads1 = L_model_backward(parameters1, caches1, X1, Wp[:,:AL1.shape[0]], layers_dims[3], dZ_next=dZ, network_bridge=1)\n",
    "        #print(\"Phase 2 gradients\")\n",
    "        grads2 = L_model_backward(parameters2, caches2, X2, Wp[:,AL1.shape[0]:AL2.shape[0]+AL1.shape[0]], layers_dims[3], dZ_next=dZ, network_bridge=1)\n",
    "        #print(\"Phase 3 gradients\")\n",
    "        grads3 = L_model_backward(parameters3, caches3, X3, Wp[:,AL2.shape[0]+AL1.shape[0]:AL3.shape[0]+AL2.shape[0]+AL1.shape[0]], layers_dims[3], dZ_next=dZ, network_bridge=1)\n",
    "\n",
    "        # Update parameters.\n",
    "        #print(\"Phase 1 updates\")\n",
    "        parameters1 = update_parameters(parameters1, grads1, layers_dims[3], learning_rate)\n",
    "        #print(\"Phase 2 updates\")\n",
    "        parameters2 = update_parameters(parameters2, grads2, layers_dims[3], learning_rate)\n",
    "        #print(\"Phase 3 updates\")\n",
    "        parameters3 = update_parameters(parameters3, grads3, layers_dims[3], learning_rate)\n",
    "        \n",
    "        #print(\"Final Layer updates\")\n",
    "        parametersE = update_parameters(parametersE, gradsE, layere_dims[3], learning_Erate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "    #print(\"Outside of Update for loop\")\n",
    "    print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "24c57744484cbad29a08ea0f84cb6d30f90a8f8c"
   },
   "outputs": [],
   "source": [
    "#ph1_train, Y1 = filetondarray(\"../train_batches/ph1_1.csv\", \"../train_batches/y1_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0c5fe8a39647dd34a30ed68f1b91c992ca74949d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ph1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1db03e9a638e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ph1 read: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mph1_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y1 read: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mph1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ph1_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"ph1 read: \", ph1_train[0:10])\n",
    "print(\"Y1 read: \", Y1[0:10])\n",
    "print(ph1_train.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ec8e09a964d7c057846b99a89674b6ff298dc4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4', 'W5', 'b5', 'W6', 'b6'])\n",
      "(50, 100)\n",
      "(50, 50)\n",
      "(50, 50)\n",
      "(12, 50)\n"
     ]
    }
   ],
   "source": [
    "layers = [100, 12, 50, 6]\n",
    "parameters = initialize_parameters_deep(layers)\n",
    "print(parameters.keys())\n",
    "print(parameters['W1'].shape)\n",
    "print(parameters['W2'].shape)\n",
    "print(parameters['W3'].shape)\n",
    "print(parameters['W6'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "52943fb4e98c6f2dab64b05d6cde9691ca8f2065",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_cleanup()\n",
    "\n",
    "#segment_traindata(2, batchsize_max)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c0b34a0197b95dd1b0cef414398526cd1ce2b345"
   },
   "outputs": [],
   "source": [
    "#ph1_train, Y1 = filetondarray(\"../train_batches/ph1_1.csv\", \"../train_batches/y1_1.csv\")\n",
    "#ph2_train, Y2 = filetondarray(\"../train_batches/ph2_1.csv\", \"../train_batches/y2_1.csv\")\n",
    "#ph3_train, Y3 = filetondarray(\"../train_batches/ph3_1.csv\", \"../train_batches/y3_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c884d02fd67914cec7eabc38f73354c4b27a2135"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ph1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-faeb2309bc39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mph1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ph1_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(ph1_train.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9c85d3d133f85d01b04d407aee7ad49e77db5dc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "e8cc36a18e4be69f1cd9422a4c60ad51ceb58920"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ph1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-faeb2309bc39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mph1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ph1_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(ph1_train.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "cec4c3d2cdf677e9bd8e9f4bc0cd628565af81c4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#parameters = L_layer_model(ph1_train, Y1, layers_dims, num_iterations = 1000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "cfbf33837176275d9a4062389265f926041e0e8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameters = L_triplelayer_model(ph1_train, ph2_train, ph3_train, Y1, layers_dims, layere_dims, learning_rate = 0.01, learning_Erate = 0.02, num_iterations = 1000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "33dda4e1333a202d9bb67ed137eb4967435244c4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
